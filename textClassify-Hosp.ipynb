{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk \n",
    "import pickle\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "string.punctuation += '=+-'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = string.punctuation.index('.')\n",
    "string.punctuation = string.punctuation[0:indexs]+string.punctuation[indexs+1:]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    #print(text)\n",
    "    no_punc = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punc \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    #print(text)\n",
    "    words = [c for c in text if c not in stopwords.words('english')]\n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def countWords(s):\n",
    "    words = s.split()\n",
    "    if len([word for word in words if len(word)<=3])/ len(s.split()) >0.50:\n",
    "        return  np.NaN\n",
    "    else :\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, \n",
    "                feature_vector_train, \n",
    "                label, \n",
    "                feature_vector_valid, \n",
    "                fname,is_neural_net=False,):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    print(confusion_matrix(predictions,valid_y))\n",
    "    fname += '.pkl'\n",
    "    pickle.dump(classifier, open(fname, 'wb'))\n",
    " \n",
    "    return accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_lemmetizer(text):\n",
    "    lem_text =\" \".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"hospital14.csv\"\n",
    "path2 = r\"hospital7.csv\"\n",
    "path3 = r\"hospital8.csv\"\n",
    "train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path1, path2 = None):\n",
    "    if path2 is not  None :\n",
    "        df1 = pd.read_csv(path1)\n",
    "        df2 = pd.read_csv(path2)\n",
    "        df = df1.append(df2, ignore_index = True)\n",
    "    else :\n",
    "        df = pd.read_csv(path1)\n",
    "    df.drop(df[df['text'].isna()].index, inplace= True)\n",
    "    df.drop(df[df['label'].isna()].index, inplace= True)\n",
    "    print(len(df))\n",
    "    print(df[df['text'].isna()].index)\n",
    "    ### removing the common start  of the document \n",
    "    dfk= df['text'].str.split(\"\\n\", n = 5, expand = True) \n",
    "    df['ptext'] = dfk[5]\n",
    "    df.drop(df[df['ptext'].isna()].index, inplace= True)\n",
    "    del dfk\n",
    "    ####\n",
    "    ## check for the substring\n",
    "    index = df.sample().index[0]\n",
    "    if df.loc[index]['ptext'] in df.loc[index]['text']:\n",
    "        print('substring fouund')\n",
    "    else :\n",
    "        print('index problem')\n",
    "        exit()\n",
    "    df['ptext'] = df['ptext'].apply(lambda x: remove_punctuation(x) )\n",
    "    df['ptext'] = df['ptext'].apply(lambda x: tokenizer.tokenize(x.lower()) )\n",
    "    df['ptext'] = df['ptext'].apply(lambda x: remove_stopwords(x))\n",
    "    df['ptext'] = df['ptext'].apply(lambda x: word_lemmetizer(x))\n",
    "    corpus = df['ptext'].tolist()\n",
    "    df_train_nlp=pd.DataFrame({'image_name':df['image_name'],'text':corpus,'label':df['label']})\n",
    "    df_train_nlp = df_train_nlp.replace(['Gaurantee_Letter','gaurantee-Letter','Guarantee Letter','Gaurantee_letter','Garauntee_Letter'],'Authorisation_Form')\n",
    "    df_train_nlp = df_train_nlp.replace(['Lab-report','Lab Report'],'Lab_Report')\n",
    "    df_train_nlp = df_train_nlp.replace(['Doctors Notes'],'Doctors_Notes')\n",
    "    return df_train_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "Int64Index([], dtype='int64')\n",
      "substring fouund\n",
      "412\n",
      "Int64Index([], dtype='int64')\n",
      "substring fouund\n"
     ]
    }
   ],
   "source": [
    "df_train =  preprocess(path1, path2 )\n",
    "df_val =  preprocess(path3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(df_train['text'],\n",
    "                                                  df_train['label'],\n",
    "                                                  test_size=0.10\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'4': 'Lab_Report',\n",
       "             '3': 'Invoice',\n",
       "             '2': 'IC',\n",
       "             '1': 'Doctor_Note',\n",
       "             '0': 'Authorisation_Form'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index_label = defaultdict(str)\n",
    "if train == True :\n",
    "    print('Train')\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    train_y = encoder.fit_transform(train_y)\n",
    "    valid_y = encoder.fit_transform(valid_y)\n",
    "    label_name = encoder.inverse_transform(train_y)   \n",
    "    for i,j in zip(train_y, label_name):\n",
    "    #for i,j in zip(valid_y, label_name):\n",
    "        if not index_label[str(i)]:\n",
    "            index_label[str(i)] = j\n",
    "if train == False :\n",
    " \n",
    "    \n",
    "    valid_y = encoder.fit_transform(df_train_nlp['label'])\n",
    "    valid_x = df_train_nlp['text']\n",
    "#     label_name = encoder.inverse_transform(valid_y)   \n",
    "#     for i,j in zip(valid_y, label_name):\n",
    "#         if not index_label[str(i)]:\n",
    "#             index_label[str(i)] = j\n",
    "    \n",
    "index_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', stop_words=\"english\", token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df_train['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word',\n",
    "                             stop_words=\"english\",\n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             max_features=5000)\n",
    "tfidf_vect.fit(df_train['text'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', \n",
    "                                   stop_words=\"english\",\n",
    "                                   token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   max_features=5000)\n",
    "tfidf_vect_ngram.fit(df_train['text'])\n",
    "xtrain_tfidf_ngram =tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', \n",
    "                                         #stop_words=\"english\",\n",
    "                                         #token_pattern=r'\\w{1,}', \n",
    "                                         ngram_range=(2,3), \n",
    "                                         max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df_train['text'])\n",
    "xtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  2  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "NB Count Vectors:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(MultinomialNB(), \n",
    "                       xtrain_count, \n",
    "                       train_y, \n",
    "                       xvalid_count,'NBcount')\n",
    "print (\"NB Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "NB accuracy with WordLevel TF-IDF:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(MultinomialNB(), \n",
    "                       xtrain_tfidf, \n",
    "                       train_y, \n",
    "                       xvalid_tfidf,'NBtfidf')\n",
    "print (\"NB accuracy with WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1 31  1]\n",
      " [ 1  0  1  0 17]]\n",
      "NB accuracy with N-Gram Vectors:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(), \n",
    "                       xtrain_tfidf_ngram, \n",
    "                       train_y, \n",
    "                       xvalid_tfidf_ngram,'NBngram')\n",
    "print (\"NB accuracy with N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0  0  0]\n",
      " [ 1 18  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1 31  1]\n",
      " [ 0  0  0  0 17]]\n",
      "NB accuracy with CharLevel Vectors:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(MultinomialNB(), \n",
    "                       xtrain_tfidf_ngram_chars, \n",
    "                       train_y, \n",
    "                       xvalid_tfidf_ngram_chars,'NBChar')\n",
    "print (\"NB accuracy with CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "model score: 0.9474\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  2  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "SVC(C=0.015, break_ties=False, cache_size=200, class_weight='balanced',\n",
      "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.4079\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 1  5  2 21 18]\n",
      " [ 0  0  0 10  0]\n",
      " [ 0  0  0  0  0]]\n",
      "===================================\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=None,\n",
      "                solver='auto', tol=0.001)\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "SGDClassifier(alpha=0.001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='squared_loss',\n",
      "              max_iter=200, n_iter_no_change=5, n_jobs=None,\n",
      "              penalty='elasticnet', power_t=0.5, random_state=7, shuffle=True,\n",
      "              tol=None, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.2368\n",
      "[[ 6 13  0 14  8]\n",
      " [ 1  1  2  5  2]\n",
      " [ 0  2  0  2  1]\n",
      " [ 0  1  0  7  3]\n",
      " [ 0  1  0  3  4]]\n",
      "===================================\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                  max_features=1.0, max_samples=1.0, n_estimators=20,\n",
      "                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                  warm_start=False)\n",
      "model score: 0.9605\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  1  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 30  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.05,\n",
      "                   n_estimators=50, random_state=None)\n",
      "model score: 0.9474\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 29  1]\n",
      " [ 1  0  0  2 17]]\n",
      "===================================\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.05, loss='deviance', max_depth=10,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=6,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
      "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=3,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "model score: 0.9605\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 30  1]\n",
      " [ 1  0  0  1 17]]\n",
      "===================================\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n",
      "[15:10:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { class_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', class_weight='balanced',\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              gamma=0, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
      "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "model score: 0.9737\n",
      "[[ 6  0  0  0  0]\n",
      " [ 0 18  0  0  0]\n",
      " [ 0  0  2  0  0]\n",
      " [ 0  0  0 31  1]\n",
      " [ 1  0  0  0 17]]\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel=\"rbf\", C=0.015, probability=True, class_weight='balanced'),\n",
    "    RidgeClassifier(tol=1e-3),\n",
    "    LogisticRegression(n_jobs=1, C=1e5, max_iter=1000, class_weight='balanced'),\n",
    "    SGDClassifier(loss='squared_loss', penalty='elasticnet', \n",
    "                  alpha=1e-3, random_state=7, \n",
    "                  max_iter=200, tol=None,\n",
    "                 class_weight='balanced'),\n",
    "        BaggingClassifier(n_estimators=20),\n",
    "    AdaBoostClassifier(n_estimators=50, learning_rate=0.05),\n",
    "    GradientBoostingClassifier(learning_rate=0.05, \n",
    "                               n_estimators=100,\n",
    "                               min_samples_split=6,\n",
    "                               max_depth=10,),\n",
    "    DecisionTreeClassifier(max_depth=20,\n",
    "                          min_samples_split=3,\n",
    "                          min_samples_leaf=1,\n",
    "                          class_weight='balanced'),\n",
    "    RandomForestClassifier(n_estimators=200,\n",
    "                           class_weight='balanced'),\n",
    "    XGBClassifier(learning_rate =0.05,\n",
    "                         n_estimators=100,\n",
    "                         class_weight='balanced')]\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('vect', CountVectorizer(max_df=0.95, \n",
    "                                                    min_df=0.,\n",
    "                                                    max_features=1200)),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "                           ('classifier', classifier)])\n",
    "    \n",
    "    pipe.fit(train_x, train_y)   \n",
    "    predtn = pipe.predict(valid_x)\n",
    "    print(classifier)\n",
    "    print(\"model score: %.4f\" % pipe.score(valid_x, valid_y))\n",
    "    print(confusion_matrix(predtn, valid_y))\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = encoder.fit_transform(df_val['label'])\n",
    "valid_x = df_val['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88   0   0   0   0]\n",
      " [ 15 108   1   0   0]\n",
      " [  1   0  15   0   0]\n",
      " [  0   0   1  81   0]\n",
      " [  0   0   0   2 100]] \n",
      " [[104   0   0   0   0]\n",
      " [  0 108   0   0   0]\n",
      " [  0   0  17   0   0]\n",
      " [  0   0   0  83   0]\n",
      " [  0   0   0   0 100]]\n",
      "0.9514563106796117\n"
     ]
    }
   ],
   "source": [
    "## final model ##\n",
    "classifier =  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
    "                     weights='uniform')\n",
    "\n",
    "model = Pipeline(steps=[('vect', CountVectorizer(max_df=0.95, \n",
    "                                                min_df=0.,\n",
    "                                                max_features=1200)),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('classifier', classifier)])\n",
    "    \n",
    "model.fit(train_x, train_y)   \n",
    "\n",
    "# save the model to disk\n",
    "filename = 'hosp_text.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(valid_x, valid_y)\n",
    "pred = loaded_model.predict(valid_x)\n",
    "print(confusion_matrix(pred,valid_y),\"\\n\", confusion_matrix(valid_y, valid_y))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.to_csv(r'C:\\task\\779\\csv\\8\\final8.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
